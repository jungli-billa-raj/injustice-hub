## ğŸ“… Day 1 â€” What we build **today** (nothing more, nothing less)

### ğŸ¯ **Goal for today**

> **Collect and store raw news articles in a local SQLite database.**

No intelligence.
No LLM.
No FastAPI.
No frontend.

Just **reliable raw data**.

---

## ğŸ§± What we will build today

### 1ï¸âƒ£ Project skeleton (minimal)

```
injustice-hub/
â”œâ”€â”€ scraper/
â”‚   â””â”€â”€ injustice_scraper/
â”‚       â”œâ”€â”€ spiders/
â”‚       â”‚   â””â”€â”€ news_spider.py
â”‚       â””â”€â”€ settings.py
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ data/
â”‚   â””â”€â”€ injustice.db
â””â”€â”€ pyproject.toml
```

---

### 2ï¸âƒ£ SQLite database (raw storage only)

**Table: `raw_articles`**

| Column       | Purpose          |
| ------------ | ---------------- |
| id           | Primary key      |
| source       | News source name |
| url          | Article URL      |
| headline     | Title            |
| published_at | Date             |
| full_text    | Article content  |
| scraped_at   | Timestamp        |

---

### 3ï¸âƒ£ Scrapy spider (ONE source only)

* Scrape:

  * headline
  * publication date
  * article text
  * URL
* Insert directly into SQLite
* No filtering, no classification

---

### 4ï¸âƒ£ Tooling rules

* Python only
* Dependency management: **`uv`**
* Database: **SQLite**
* Scraper: **Scrapy**

---

## âœ… End-of-day success criteria

By the end of **Day 1**, you should be able to:

* Run one command
* Scrape real news articles
* See rows inserted into `raw_articles`

If **this works**, we move to Day 2.
If it doesnâ€™t, **we fix nothing else**.

---

## â­ï¸ Next step (your call)

Say **one** of these:

* â€œCreate SQLite schemaâ€
* â€œSet up uv projectâ€
* â€œWrite Scrapy spiderâ€
* â€œChoose news sourceâ€


